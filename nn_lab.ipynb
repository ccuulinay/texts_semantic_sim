{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./data/train.txt\"\n",
    "test_file = \"./data/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data file\n",
    "train_df = pd.read_csv(train_file, sep=\"\\t\", header=None, names=[\"q1\", \"q2\", \"label\"])\n",
    "test_df = pd.read_csv(test_file, sep=\"\\t\", header=None, names=[\"q1\", \"q2\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.728 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# Read in stopwords from web EDA\n",
    "with open(\"./data/stop_words.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    stop_words_list = [line.strip() for line in f]\n",
    "    \n",
    "# Read in spelling correction from web EDA\n",
    "with open(\"./data/spelling_corrections.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    spell_chk = json.loads(f.read())\n",
    "    \n",
    "import jieba\n",
    "jieba.load_userdict(\"./data/dict_all.txt\")\n",
    "\n",
    "def preprocessing_n_seq(text):\n",
    "    for token_str,replac_str in spell_chk.items():\n",
    "        text = text.replace(token_str, replac_str)\n",
    "        \n",
    "    tokens = [t for t in jieba.cut(text.strip()) if t not in stop_words_list]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 54.8 ms, total: 14 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df['q1_tokens'] = train_df['q1'].apply(lambda x: preprocessing_n_seq(x))\n",
    "train_df['q2_tokens'] = train_df['q2'].apply(lambda x: preprocessing_n_seq(x))\n",
    "test_df['q1_tokens'] = test_df['q1'].apply(lambda x: preprocessing_n_seq(x))\n",
    "test_df['q2_tokens'] = test_df['q2'].apply(lambda x: preprocessing_n_seq(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in tencent pretrained word vector\n",
    "tencent_pretrained_vec_loc = \"/data/Tencent_AILab_ChineseEmbedding.txt\"\n",
    "def process_raw_fileline(line, number_dim):\n",
    "    line = line.rstrip().decode('utf8')\n",
    "    pieces = line.rsplit(' ', int(number_dim))\n",
    "    word = pieces[0]\n",
    "    vector = np.asarray([float(v) for v in pieces[1:]], dtype='f')\n",
    "    return word, vector\n",
    "\n",
    "def load_tencent_pretrained_vectors():\n",
    "    from multiprocessing import Pool  # For CPU\n",
    "    from multiprocessing.dummy import Pool as ThreadPool  # For IO\n",
    "    from functools import partial\n",
    "\n",
    "    with open(tencent_pretrained_vec_loc, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        number_row, number_dim = header.split()\n",
    "\n",
    "        pool = Pool(8)\n",
    "        # pool = ThreadPool(8)\n",
    "        process_raw_fileline_100dim = partial(process_raw_fileline, number_dim=number_dim)\n",
    "        wv_set_list = pool.map(process_raw_fileline_100dim, f)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        # print(\"pool done.\")\n",
    "        w2v = dict(wv_set_list)\n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.5 s, sys: 35.5 s, total: 1min 19s\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tencent_w2v = load_tencent_pretrained_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tencent_w2v[\"æˆ‘\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    50220\n",
      "0    50220\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Try to handle the imbalance data\n",
    "# Upsample data with label == 1\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "ori_train_df = train_df.copy()\n",
    "ready_to_upsampled_df = train_df.copy()\n",
    "\n",
    "df_majority = ready_to_upsampled_df[ready_to_upsampled_df.label==0]\n",
    "df_minority = ready_to_upsampled_df[ready_to_upsampled_df.label==1]\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=df_majority.shape[0],    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "upsampled_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "print(upsampled_df.label.value_counts())\n",
    "\n",
    "train_df = upsampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import TransformerMixin\n",
    "import jieba\n",
    "\n",
    "jieba.load_userdict(\"./data/dict_all.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 20000 8824330\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = np.shape(next(iter(tencent_w2v.values())))[0]\n",
    "MAX_NUM_WORDS = 20000\n",
    "print(EMBEDDING_DIM, MAX_NUM_WORDS, len(tencent_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(raw_texts):\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    def bs_unescape_html(text):\n",
    "        return bs(text, \"lxml\").get_text()\n",
    "    def preprocess_text(raw_text):\n",
    "        # text = re.sub(r'\\(.*\\)', '', raw_text)\n",
    "        text = \" \".join(jieba.cut(raw_text))\n",
    "        return bs_unescape_html(text.lower().strip())\n",
    "    \n",
    "    return [preprocess_text(t) for t in raw_texts]\n",
    "\n",
    "\n",
    "class Text2SeqTransformer(Tokenizer, BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        params: num_words: to be a vocab_size of returning tokenzier and dictionary.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def fit(self, texts, y=None):\n",
    "        \"\"\"\n",
    "        params: texts: list of strings\n",
    "        \"\"\"\n",
    "        self.fit_on_texts(texts)\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts, y=None):\n",
    "        return np.array(self.texts_to_sequences(texts))\n",
    "    \n",
    "# Implement a padder transformer base on keras pad_sequences\n",
    "class PaddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, maxlen=200):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_index = None\n",
    "    def fit(self, X, y=None):\n",
    "        self.max_index = pad_sequences(X, maxlen=self.maxlen).max()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = pad_sequences(X, maxlen=self.maxlen)\n",
    "        X[X > self.max_index] = 0\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build preprocess pipeline\n",
    "import re\n",
    "from sklearn.pipeline import make_pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "max_words = 200\n",
    "vocab_size = len(tencent_w2v)\n",
    "\n",
    "preprocess_pipeline = make_pipeline(\n",
    "    FunctionTransformer(preprocess_texts, validate=False)\n",
    "    , Text2SeqTransformer(num_words=MAX_NUM_WORDS)\n",
    "    , PaddingTransformer(maxlen=max_words)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.8 s, sys: 414 ms, total: 56.3 s\n",
      "Wall time: 59.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('functiontransformer',\n",
       "                 FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "                                     func=<function preprocess_texts at 0x7fb62e13e268>,\n",
       "                                     inv_kw_args=None, inverse_func=None,\n",
       "                                     kw_args=None, pass_y='deprecated',\n",
       "                                     validate=False)),\n",
       "                ('text2seqtransformer', Text2SeqTransformer()),\n",
       "                ('paddingtransformer', PaddingTransformer(maxlen=200))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "preprocess_pipeline.fit(list(train_df['q1']) + list(train_df['q2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100440, 200)\n"
     ]
    }
   ],
   "source": [
    "x_left = preprocess_pipeline.transform(train_df['q1'])\n",
    "x_right = preprocess_pipeline.transform(train_df['q2'])\n",
    "# this will be the input of the siamese network\n",
    "x_pairs = [x_left, x_right]   \n",
    "\n",
    "y_pairs = train_df['label'].values\n",
    "print(np.shape(x_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Subtract, Conv1D, Flatten, Embedding, LSTM, GRU, Bidirectional\n",
    "from keras.layers import BatchNormalization, concatenate, Concatenate, Multiply, AveragePooling1D, MaxPooling1D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10269\n"
     ]
    }
   ],
   "source": [
    "# get word dict\n",
    "tokenize_texts = preprocess_texts(list(train_df['q1']) + list(train_df['q2']))\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(tokenize_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(w2v):\n",
    "    num_words = min(MAX_NUM_WORDS, len(w2v) + 1) if MAX_NUM_WORDS else len(w2v) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = w2v.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_embeddings_loop():\n",
    "    num_words = min(MAX_NUM_WORDS, len(w2v) + 1) if MAX_NUM_WORDS else len(w2v) + 1\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = w2v.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_keras_embeddings_layers(w2v, maxlen):\n",
    "    from keras.layers import Embedding\n",
    "    embeddings = get_embeddings(w2v)\n",
    "    x_embedded = Embedding(\n",
    "        embeddings.shape[0]\n",
    "        , embeddings.shape[1]\n",
    "        , input_length=maxlen\n",
    "        , trainable=False\n",
    "        , weights=[embeddings]\n",
    "    )\n",
    "    return x_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(difference):\n",
    "    \"\"\" Compute the exponent of the opposite of the L1 norm of a vector, to get the left/right inputs\n",
    "    similarity from the inputs differences. This function is used to turned the unbounded\n",
    "    L1 distance to a similarity measure between 0 and 1\"\"\"\n",
    "    return K.exp(-K.sum(K.abs(difference), axis=1, keepdims=True))\n",
    "\n",
    "def cosine_distance(inputs):\n",
    "    return K.sum(inputs[0] * inputs[1],axis=1,keepdims=True)/(K.sum(inputs[0]**2,axis=1,keepdims=True) * K.sum(inputs[1]**2,axis=1,keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_lstm(max_length, embedding_layer):\n",
    "    \"\"\" Define, compile and return a siamese LSTM model \"\"\"\n",
    "    input_shape = (max_length,)\n",
    "    left_input = Input(input_shape, name='left_input')\n",
    "    right_input = Input(input_shape, name='right_input')\n",
    "\n",
    "    # Define a single sequential model for both arms.\n",
    "    # In this example I've chosen a simple bidirectional LSTM with no dropout\n",
    "    seq = Sequential(name='sequential_network')\n",
    "    seq.add(embedding_layer)\n",
    "    seq.add(Bidirectional(LSTM(32, dropout=0., recurrent_dropout=0.)))\n",
    "    \n",
    "    left_output = seq(left_input)\n",
    "    right_output = seq(right_input)\n",
    "\n",
    "    # Here we subtract the neuron values of the last layer from the left arm \n",
    "    # with the corresponding values from the right arm\n",
    "    subtracted = Subtract(name='pair_representations_difference')([left_output, right_output])\n",
    "    \n",
    "    # 1 This is exponent negative manhattan distance\n",
    "    \n",
    "    manhattan_lstm_distance = Lambda(exponent_neg_manhattan_distance, name='masltsm_distance')(subtracted)\n",
    "    \n",
    "    # 2 This is sigmoid \n",
    "    L1_layer = Lambda(lambda tensors: K.abs(tensors))\n",
    "    L1_distance = L1_layer(subtracted)\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "    \n",
    "    # 3 Use bn and dense to make the distance.\n",
    "    \n",
    "    concated = concatenate([left_output, right_output])\n",
    "    concated = BatchNormalization()(concated)\n",
    "    concated = Dropout(0.3)(concated)\n",
    "    concated = Dense(64, activation='relu')(concated)\n",
    "    concated = BatchNormalization()(concated)\n",
    "    concated = Dropout(0.3)(concated)\n",
    "    preds = Dense(1, activation='sigmoid')(concated)\n",
    "\n",
    "    \n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=manhattan_lstm_distance)\n",
    "    # siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "    # siamese_net = Model(inputs=[left_input, right_input], outputs=preds)\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = get_keras_embeddings_layers(tencent_w2v, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_epo_siamese_lstm = siamese_lstm(max_words, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_input (InputLayer)         (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_input (InputLayer)        (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_network (Sequential) (None, 64)           4059648     left_input[0][0]                 \n",
      "                                                                 right_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pair_representations_difference (None, 64)           0           sequential_network[1][0]         \n",
      "                                                                 sequential_network[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "masltsm_distance (Lambda)       (None, 1)            0           pair_representations_difference[0\n",
      "==================================================================================================\n",
      "Total params: 4,059,648\n",
      "Trainable params: 59,648\n",
      "Non-trainable params: 4,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "one_epo_siamese_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90396 samples, validate on 10044 samples\n",
      "Epoch 1/2\n",
      "90396/90396 [==============================] - 903s 10ms/step - loss: 0.5982 - accuracy: 0.6860 - val_loss: 0.7185 - val_accuracy: 0.5198\n",
      "Epoch 2/2\n",
      "90396/90396 [==============================] - 903s 10ms/step - loss: 0.5520 - accuracy: 0.7354 - val_loss: 0.7363 - val_accuracy: 0.5107\n",
      "CPU times: user 1h 8min 38s, sys: 5min 31s, total: 1h 14min 10s\n",
      "Wall time: 30min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f2afe228d68>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "one_epo_siamese_lstm.fit(x_pairs, y_pairs, validation_split=0.1, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 30744, 200)\n"
     ]
    }
   ],
   "source": [
    "x_test_left = preprocess_pipeline.transform(test_df['q1'])\n",
    "x_test_right = preprocess_pipeline.transform(test_df['q2'])\n",
    "# this will be the input of the siamese network\n",
    "x_test_pairs = [x_test_left, x_test_right]   \n",
    "\n",
    "y_test_pairs = test_df['label'].values\n",
    "print(np.shape(x_test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_run_x_test_pairs = [x_test_left[:3], x_test_right[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.47568128],\n",
       "       [0.24673058],\n",
       "       [0.14412421]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_test_pairs[:3])\n",
    "one_epo_siamese_lstm.predict(dry_run_x_test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = one_epo_siamese_lstm.evaluate(x_test_pairs, y_test_pairs, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss, accuracy:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4546530999896615, 0.8186963200569153]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"loss, accuracy:\\n\".format(score))\n",
    "[0.4546530999896615, 0.8186963200569153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prop = one_epo_siamese_lstm.predict(x_test_pairs)\n",
    "y_pred = np.where(y_prop > 0.5, 1, 0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22562  2607]\n",
      " [ 3144  2431]]\n",
      "\n",
      "F1 score 0.4581174031847734\n",
      "Accuracy 0.8129391100702577\n",
      "ROC AUC SCORE 0.6662370055554496\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89     25169\n",
      "           1       0.48      0.44      0.46      5575\n",
      "\n",
      "    accuracy                           0.81     30744\n",
      "   macro avg       0.68      0.67      0.67     30744\n",
      "weighted avg       0.81      0.81      0.81     30744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test_pairs, y_pred)  \n",
    "print(cm)  \n",
    "print()\n",
    "print(\"F1 score\", f1_score(y_test_pairs, y_pred))\n",
    "print('Accuracy', accuracy_score(y_test_pairs, y_pred))\n",
    "print('ROC AUC SCORE', roc_auc_score(y_test_pairs, y_pred))\n",
    "print(classification_report(y_test_pairs, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "manhattan_lstm_distance (imbalanced)\n",
    "[[24975   194]\n",
    " [ 5157   418]]\n",
    "\n",
    "F1 score 0.1351220300630354\n",
    "Accuracy 0.8259497788186313\n",
    "ROC AUC SCORE 0.5336348419215253\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.99      0.90     25169\n",
    "           1       0.68      0.07      0.14      5575\n",
    "\n",
    "    accuracy                           0.83     30744\n",
    "   macro avg       0.76      0.53      0.52     30744\n",
    "weighted avg       0.80      0.83      0.76     30744\n",
    "'''\n",
    "\n",
    "'''\n",
    "manhattan_lstm_distance (upsampled)\n",
    "\n",
    "[[22562  2607]\n",
    " [ 3144  2431]]\n",
    "\n",
    "F1 score 0.4581174031847734\n",
    "Accuracy 0.8129391100702577\n",
    "ROC AUC SCORE 0.6662370055554496\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.90      0.89     25169\n",
    "           1       0.48      0.44      0.46      5575\n",
    "\n",
    "    accuracy                           0.81     30744\n",
    "   macro avg       0.68      0.67      0.67     30744\n",
    "weighted avg       0.81      0.81      0.81     30744\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "bn and dense to make the distance.\n",
    "[[25162     7]\n",
    " [ 5556    19]]\n",
    "\n",
    "F1 score 0.006784502767362971\n",
    "Accuracy 0.8190541243819932\n",
    "ROC AUC SCORE 0.5015649759197333\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.82      1.00      0.90     25169\n",
    "           1       0.73      0.00      0.01      5575\n",
    "\n",
    "    accuracy                           0.82     30744\n",
    "   macro avg       0.77      0.50      0.45     30744\n",
    "weighted avg       0.80      0.82      0.74     30744\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_gru_nn(max_length, embedding_layer):\n",
    "    \n",
    "    input_shape = (max_length,)\n",
    "    left_input = Input(input_shape, name='left_input')\n",
    "    right_input = Input(input_shape, name='right_input')\n",
    "\n",
    "    # Define a single sequential model for both arms.\n",
    "    seq = Sequential(name='sequential_network')\n",
    "    seq.add(embedding_layer)\n",
    "    seq.add(Conv1D(16, kernel_size=3, activation='relu'))\n",
    "    seq.add(AveragePooling1D(2))\n",
    "    seq.add(MaxPooling1D(2))\n",
    "    seq.add(Bidirectional(GRU(32, dropout=0., recurrent_dropout=0.)))\n",
    "    \n",
    "    left_output = seq(left_input)\n",
    "    right_output = seq(right_input)\n",
    "\n",
    "    # Here we subtract the neuron values of the last layer from the left arm \n",
    "    # with the corresponding values from the right arm\n",
    "    subtracted = Subtract(name='pair_representations_difference')([left_output, right_output])\n",
    "    \n",
    "    \n",
    "    manhattan_distance = Lambda(exponent_neg_manhattan_distance, name='manh_distance')(subtracted)\n",
    "    cos_dist = Lambda(cosine_distance, name='cosine_distance')([left_output, right_output])\n",
    "    \n",
    "    L1_layer = Lambda(lambda tensors: K.abs(tensors))\n",
    "    L1_distance = L1_layer(subtracted)\n",
    "    \n",
    "    l_r_mul = Multiply(name=\"pair_rep_multiply\")([left_output, right_output])\n",
    "    \n",
    "    \n",
    "    concated = Concatenate(axis=1)([L1_distance, l_r_mul, manhattan_distance, cos_dist])\n",
    "    concated = Dropout(0.05)(concated)\n",
    "    concated = BatchNormalization()(concated)\n",
    "    concated = Dropout(0.05)(concated)\n",
    "    concated = Dense(64, activation='relu')(concated)\n",
    "    concated = BatchNormalization()(concated)\n",
    "    concated = Dropout(0.05)(concated)\n",
    "    preds = Dense(1, activation='sigmoid')(concated)\n",
    "    \n",
    "    _net = Model(inputs=[left_input, right_input], outputs=preds)\n",
    "    _net.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "    return _net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn = siamese_gru_nn(max_words, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_input (InputLayer)         (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_input (InputLayer)        (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_network (Sequential) (None, 64)           4019024     left_input[0][0]                 \n",
      "                                                                 right_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pair_representations_difference (None, 64)           0           sequential_network[1][0]         \n",
      "                                                                 sequential_network[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 64)           0           pair_representations_difference[0\n",
      "__________________________________________________________________________________________________\n",
      "pair_rep_multiply (Multiply)    (None, 64)           0           sequential_network[1][0]         \n",
      "                                                                 sequential_network[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "manh_distance (Lambda)          (None, 1)            0           pair_representations_difference[0\n",
      "__________________________________________________________________________________________________\n",
      "cosine_distance (Lambda)        (None, 1)            0           sequential_network[1][0]         \n",
      "                                                                 sequential_network[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 130)          0           lambda_1[0][0]                   \n",
      "                                                                 pair_rep_multiply[0][0]          \n",
      "                                                                 manh_distance[0][0]              \n",
      "                                                                 cosine_distance[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 130)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 130)          520         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 130)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8384        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64)           256         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            65          dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,028,249\n",
      "Trainable params: 27,861\n",
      "Non-trainable params: 4,000,388\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90396 samples, validate on 10044 samples\n",
      "Epoch 1/1\n",
      "90396/90396 [==============================] - 287s 3ms/step - loss: 0.5839 - accuracy: 0.6846 - val_loss: 0.6764 - val_accuracy: 0.6314\n",
      "CPU times: user 10min 27s, sys: 48.4 s, total: 11min 15s\n",
      "Wall time: 4min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb78b1e2828>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "simple_nn.fit(x_pairs, y_pairs, validation_split=0.1, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prop = simple_nn.predict(x_test_pairs)\n",
    "y_pred = np.where(y_prop > 0.5, 1, 0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18389  6780]\n",
      " [ 2895  2680]]\n",
      "\n",
      "F1 score 0.35650149650814766\n",
      "Accuracy 0.685304449648712\n",
      "ROC AUC SCORE 0.6056692454077699\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.73      0.79     25169\n",
      "           1       0.28      0.48      0.36      5575\n",
      "\n",
      "    accuracy                           0.69     30744\n",
      "   macro avg       0.57      0.61      0.57     30744\n",
      "weighted avg       0.76      0.69      0.71     30744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test_pairs, y_pred)  \n",
    "print(cm)  \n",
    "print()\n",
    "print(\"F1 score\", f1_score(y_test_pairs, y_pred))\n",
    "print('Accuracy', accuracy_score(y_test_pairs, y_pred))\n",
    "print('ROC AUC SCORE', roc_auc_score(y_test_pairs, y_pred))\n",
    "print(classification_report(y_test_pairs, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "simple nn with concate L1 distance, cosine distance, exp neg manhattan distance, and multiply and then MLP.\n",
    "2 epochs.\n",
    "\n",
    "Confusion matrix:\n",
    "[[14502 10667]\n",
    " [ 2013  3562]]\n",
    "\n",
    "F1 score 0.3597253080185821\n",
    "Accuracy 0.5875618006765547\n",
    "ROC AUC SCORE 0.6075543781436592\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.58      0.70     25169\n",
    "           1       0.25      0.64      0.36      5575\n",
    "\n",
    "    accuracy                           0.59     30744\n",
    "   macro avg       0.56      0.61      0.53     30744\n",
    "weighted avg       0.76      0.59      0.63     30744\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "simple nn with concate L1 distance, cosine distance, exp neg manhattan distance, and multiply and then MLP.\n",
    "6 epochs.\n",
    "\n",
    "[[17591  7578]\n",
    " [ 2686  2889]]\n",
    "\n",
    "F1 score 0.36017952873706516\n",
    "Accuracy 0.6661462399167317\n",
    "ROC AUC SCORE 0.6085608051900987\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.70      0.77     25169\n",
    "           1       0.28      0.52      0.36      5575\n",
    "\n",
    "    accuracy                           0.67     30744\n",
    "   macro avg       0.57      0.61      0.57     30744\n",
    "weighted avg       0.76      0.67      0.70     30744\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
